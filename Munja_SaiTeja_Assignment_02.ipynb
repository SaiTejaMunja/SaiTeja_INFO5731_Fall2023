{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiTejaMunja/SaiTeja_INFO5731_Fall2023/blob/main/Munja_SaiTeja_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ],
      "metadata": {
        "id": "9rvb0gPrlkJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1**"
      ],
      "metadata": {
        "id": "LyKX0LAQlnSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
      ],
      "metadata": {
        "id": "H3jqeonRO6Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "\n",
        "# Using Second Source #2\n",
        "\n",
        "# 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "# The Batman (2022) - Movie Name\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Defining the URL of the IMDb page for the film's user reviews\n",
        "film_url = \"https://www.imdb.com/title/tt1877830/reviews\"\n",
        "\n",
        "# Creating a list to store all user reviews\n",
        "all_user_reviews = []\n",
        "\n",
        "# Defining the number of pages to scrape (adjust as needed)\n",
        "num_pages_to_scrape = 800  # You can change this to collect more pages\n",
        "\n",
        "for page_num in range(1, num_pages_to_scrape + 1):\n",
        "    # Creating the URL for the current page\n",
        "    page_url = f\"{film_url}?start={((page_num - 1) * 10)}\"\n",
        "\n",
        "    # Sending an HTTP GET request to the IMDb page\n",
        "    response = requests.get(page_url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Parsing the HTML content of the page\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Finding the review elements on the page\n",
        "        review_elements = soup.find_all(\"div\", class_=\"text show-more__control\")\n",
        "\n",
        "        for review_element in review_elements:\n",
        "            review_text = review_element.get_text(strip=True)\n",
        "            all_user_reviews.append(review_text)\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve IMDb page {page_num}. Check the URL or your internet connection.\")\n",
        "\n",
        "# Creating a CSV file to store all user reviews\n",
        "with open(\"imdb_user_reviews.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Writing the header row\n",
        "    writer.writerow([\"User Review\"])\n",
        "\n",
        "    # Writing the data for each user review\n",
        "    for review in all_user_reviews:\n",
        "        writer.writerow([review])\n",
        "\n",
        "print(f\"Collected {len(all_user_reviews)} user reviews and saved to 'imdb_user_reviews.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCPS55m2U1Cr",
        "outputId": "34567f1f-fec1-41f1-937d-ed7a1f08f176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 20000 user reviews and saved to 'imdb_user_reviews.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries and reading csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "imdb = pd.read_csv('imdb_user_reviews.csv')"
      ],
      "metadata": {
        "id": "A9zfXqH5U9fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking head of the dataset\n",
        "\n",
        "imdb.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0986e976-f209-4e68-a9a1-afd6300f1369",
        "id": "IXorbJsiU9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         User Review\n",
              "0  Detective Batman at its peak! Great storyline....\n",
              "1  I just got out of The BatmanThis movie really ...\n",
              "2  A serial killer strikes in Gotham City, killin...\n",
              "3  I have been absolutely fizzing to see 'The Bat...\n",
              "4  The Riddler(Paul Dano, spot-on. How did it tak..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e5a27739-ced3-4d7d-84b8-a0c8c99725b6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Detective Batman at its peak! Great storyline....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I just got out of The BatmanThis movie really ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A serial killer strikes in Gotham City, killin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I have been absolutely fizzing to see 'The Bat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Riddler(Paul Dano, spot-on. How did it tak...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5a27739-ced3-4d7d-84b8-a0c8c99725b6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e5a27739-ced3-4d7d-84b8-a0c8c99725b6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e5a27739-ced3-4d7d-84b8-a0c8c99725b6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2329210c-03d6-4ab7-b51d-292881be3076\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2329210c-03d6-4ab7-b51d-292881be3076')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2329210c-03d6-4ab7-b51d-292881be3076 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the sape of the data\n",
        "\n",
        "imdb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeI7lDT2U_8g",
        "outputId": "f74f4c00-997a-4b6a-d5f4-10bbf15a9db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a862980-b423-4689-b18a-ec002ff30f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully cleaned and saved the text data to imdb_reviews_clean.csv\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "# importing necessary libraries\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Loading the stopwords list\n",
        "stopwords = stopwords.words(\"english\")\n",
        "\n",
        "# Defining a function to clean the text data\n",
        "def clean_text(text):\n",
        "    # Removing noise, such as special characters and punctuations\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "\n",
        "    # Removing numbers\n",
        "    text = re.sub(r\"[0-9]\", \"\", text)\n",
        "\n",
        "    # Removing stopwords\n",
        "    text = \" \".join([word for word in text.split() if word not in stopwords])\n",
        "\n",
        "    # applying Lowercase to all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    # applying Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "    # applying Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "    return text\n",
        "\n",
        "# Opening the CSV file\n",
        "with open(\"imdb_user_reviews.csv\", \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader) # Skip the header row\n",
        "\n",
        "    # Creating a new CSV file to store the cleaned data\n",
        "    with open(\"imdb_reviews_clean.csv\", \"w\", newline=\"\") as f_out:\n",
        "        writer = csv.writer(f_out)\n",
        "        writer.writerow([\"Review\", \"Cleaned Review\"])\n",
        "\n",
        "        for row in reader:\n",
        "            review = row[0]\n",
        "\n",
        "            # Cleaning the review text\n",
        "            cleaned_review = clean_text(review)\n",
        "\n",
        "            # Writing the cleaned review to the new CSV file\n",
        "            writer.writerow([review, cleaned_review])\n",
        "\n",
        "# Closing the files\n",
        "f.close()\n",
        "f_out.close()\n",
        "\n",
        "print(\"Successfully cleaned and saved the text data to imdb_reviews_clean.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Write Your Code here\n",
        "\n",
        "# Importing necessary libraries\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Loading the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Defining a function to perform POS tagging and count parts of speech\n",
        "def count_parts_of_speech(text):\n",
        "    doc = nlp(text)\n",
        "    pos_counts = {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}\n",
        "\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'NOUN':\n",
        "            pos_counts['Noun'] += 1\n",
        "        elif token.pos_ == 'VERB':\n",
        "            pos_counts['Verb'] += 1\n",
        "        elif token.pos_ == 'ADJ':\n",
        "            pos_counts['Adjective'] += 1\n",
        "        elif token.pos_ == 'ADV':\n",
        "            pos_counts['Adverb'] += 1\n",
        "\n",
        "    return pos_counts\n",
        "\n",
        "# Opening  the cleaned data file\n",
        "with open(\"imdb_reviews_clean.csv\", \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # Skip the header row\n",
        "\n",
        "    # Initializing POS counts\n",
        "    total_pos_counts = {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}\n",
        "\n",
        "    for row in reader:\n",
        "        cleaned_review = row[1]\n",
        "\n",
        "        # Counting POS in the cleaned review\n",
        "        pos_counts = count_parts_of_speech(cleaned_review)\n",
        "\n",
        "        # Updating the total POS counts\n",
        "        for pos, count in pos_counts.items():\n",
        "            total_pos_counts[pos] += count\n",
        "\n",
        "# Printing the total counts of each POS\n",
        "print(\"Total Parts of Speech Counts:\")\n",
        "for pos, count in total_pos_counts.items():\n",
        "    print(f\"{pos}: {count}\")\n",
        "\n",
        "# Closing the file\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jiyqg8AHL4u",
        "outputId": "219d5406-b4cd-459a-b231-2f02ef89c645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parts of Speech Counts:\n",
            "Noun: 996000\n",
            "Verb: 526400\n",
            "Adjective: 372800\n",
            "Adverb: 144000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Write Your Code here\n",
        "\n",
        "# Importing necessary libraries\n",
        "\n",
        "\n",
        "import csv\n",
        "import spacy\n",
        "\n",
        "# Loading the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Opening the cleaned data file (imdb_reviews_clean.csv)\n",
        "with open(\"imdb_reviews_clean.csv\", \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # Skipping the header row\n",
        "\n",
        "    # Processing a single review for parsing analysis\n",
        "    for row in reader:\n",
        "        cleaned_review = row[1]  # retrieving  the cleaned review text\n",
        "\n",
        "        # Tokenizing and parsing the cleaned review\n",
        "        doc = nlp(cleaned_review)\n",
        "\n",
        "        # Constituency Parsing\n",
        "\n",
        "        print(\"Constituency Parsing Tree:\")\n",
        "        for token in doc:\n",
        "            print(f\"{token.text} [{token.dep_}] -> {token.head.text} [{token.head.dep_}]\")\n",
        "\n",
        "        # Dependency Parsing\n",
        "\n",
        "        print(\"\\nDependency Parsing Tree:\")\n",
        "        for token in doc:\n",
        "            print(f\"{token.text} [{token.dep_}] -> {token.head.text} [{token.head.dep_}]\")\n",
        "\n",
        "        # Visualizing the dependency parsing tree\n",
        "        from spacy import displacy\n",
        "        displacy.serve(doc, style=\"dep\")\n",
        "        break\n",
        "\n",
        "# Closing the file\n",
        "f.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAh7kcNnHL7c",
        "outputId": "50a7e289-4747-4873-ca33-52e7a8890720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constituency Parsing Tree:\n",
            "detect [ROOT] -> detect [ROOT]\n",
            "batman [compound] -> storylin [dobj]\n",
            "peak [nmod] -> storylin [dobj]\n",
            "great [amod] -> storylin [dobj]\n",
            "storylin [dobj] -> detect [ROOT]\n",
            "just [advmod] -> dark [amod]\n",
            "dark [amod] -> univers [dobj]\n",
            "univers [dobj] -> detect [ROOT]\n",
            "we [nsubj] -> come [relcl]\n",
            "ve [aux] -> come [relcl]\n",
            "come [relcl] -> univers [dobj]\n",
            "expect [ccomp] -> detect [ROOT]\n",
            "dc [ccomp] -> expect [ccomp]\n",
            "the [det] -> gloomi [compound]\n",
            "gloomi [compound] -> exactli [npadvmod]\n",
            "gritti [compound] -> exactli [npadvmod]\n",
            "dark [compound] -> tone [compound]\n",
            "tone [compound] -> exactli [npadvmod]\n",
            "film [compound] -> exactli [npadvmod]\n",
            "exactli [npadvmod] -> detect [ROOT]\n",
            "i [nsubj] -> want [ROOT]\n",
            "want [ROOT] -> want [ROOT]\n",
            "when [advmod] -> think [ccomp]\n",
            "think [ccomp] -> want [ROOT]\n",
            "movi [dobj] -> think [ccomp]\n",
            "there [advmod] -> movi [dobj]\n",
            "beauti [nsubj] -> cinematographi [ccomp]\n",
            "cinematographi [ccomp] -> think [ccomp]\n",
            "great [amod] -> score [dobj]\n",
            "score [dobj] -> cinematographi [ccomp]\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "detect [ROOT] -> detect [ROOT]\n",
            "batman [compound] -> storylin [dobj]\n",
            "peak [nmod] -> storylin [dobj]\n",
            "great [amod] -> storylin [dobj]\n",
            "storylin [dobj] -> detect [ROOT]\n",
            "just [advmod] -> dark [amod]\n",
            "dark [amod] -> univers [dobj]\n",
            "univers [dobj] -> detect [ROOT]\n",
            "we [nsubj] -> come [relcl]\n",
            "ve [aux] -> come [relcl]\n",
            "come [relcl] -> univers [dobj]\n",
            "expect [ccomp] -> detect [ROOT]\n",
            "dc [ccomp] -> expect [ccomp]\n",
            "the [det] -> gloomi [compound]\n",
            "gloomi [compound] -> exactli [npadvmod]\n",
            "gritti [compound] -> exactli [npadvmod]\n",
            "dark [compound] -> tone [compound]\n",
            "tone [compound] -> exactli [npadvmod]\n",
            "film [compound] -> exactli [npadvmod]\n",
            "exactli [npadvmod] -> detect [ROOT]\n",
            "i [nsubj] -> want [ROOT]\n",
            "want [ROOT] -> want [ROOT]\n",
            "when [advmod] -> think [ccomp]\n",
            "think [ccomp] -> want [ROOT]\n",
            "movi [dobj] -> think [ccomp]\n",
            "there [advmod] -> movi [dobj]\n",
            "beauti [nsubj] -> cinematographi [ccomp]\n",
            "cinematographi [ccomp] -> think [ccomp]\n",
            "great [amod] -> score [dobj]\n",
            "score [dobj] -> cinematographi [ccomp]\n",
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Write Your Code here\n",
        "\n",
        "# Importing necessary libraries\n",
        "\n",
        "import csv\n",
        "import spacy\n",
        "\n",
        "# Loading the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Opening the cleaned data file (imdb_reviews_clean.csv)\n",
        "\n",
        "with open(\"imdb_reviews_clean.csv\", \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # Skip the header row\n",
        "\n",
        "    # Initializing entity counts\n",
        "    entity_counts = {}\n",
        "\n",
        "    for row in reader:\n",
        "        cleaned_review = row[1]  # Retrieving the cleaned review text\n",
        "\n",
        "        # Extracting entities from the cleaned review\n",
        "        doc = nlp(cleaned_review)\n",
        "        for ent in doc.ents:\n",
        "            entity_type = ent.label_\n",
        "            entity_text = ent.text\n",
        "\n",
        "            if entity_type in entity_counts:\n",
        "                entity_counts[entity_type].append(entity_text)\n",
        "            else:\n",
        "                entity_counts[entity_type] = [entity_text]\n",
        "\n",
        "# Calculating the count of each entity type\n",
        "entity_type_counts = {entity_type: len(entities) for entity_type, entities in entity_counts.items()}\n",
        "\n",
        "# Printing the entity type counts\n",
        "print(\"Entity Type Counts:\")\n",
        "for entity_type, count in entity_type_counts.items():\n",
        "    print(f\"{entity_type}: {count}\")\n",
        "\n",
        "# Closing the file\n",
        "f.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAcjDnaqHL-O",
        "outputId": "8fdd0ad3-0e1c-4686-f6b4-2f0148d8ff79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity Type Counts:\n",
            "ORDINAL: 9600\n",
            "CARDINAL: 29600\n",
            "PERSON: 137600\n",
            "NORP: 8800\n",
            "ORG: 24000\n",
            "DATE: 5600\n",
            "TIME: 8800\n",
            "GPE: 12800\n",
            "PRODUCT: 800\n",
            "WORK_OF_ART: 800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) **Constituency parsing trees** use a tree structure to describe the syntactic structure of a sentence, with the nodes of the tree standing for individual phrases and the edges for the connections between those phrases. The complete phrase is represented by the tree's root node.\n",
        "\n",
        "**Dependency Parsing trees** describe the syntactic structure of a sentence as a directed graph, where the nodes of the graph represent words and the edges of the graph indicate the grammatical connections between those words. The head word of a sentence is represented by the graph's root node.\n",
        "\n",
        "**For Example:**\n",
        "\n",
        "Consider the sentence \"Everything Everywhere All at Once is the best movie of the year.\"\n",
        "\n",
        "**The constituency parsing tree for this sentence is as follows:**\n",
        "\n",
        "[S\n",
        "  [NP [NNP Everything] [NNP Everywhere] [NN All] [NN at] [NN Once]]\n",
        "  [VP [VBZ is] [JJ the] [JJS best] [NN movie] [PP [IN of] [NP [NN the] [NN year]]]]\n",
        "  [. .]]\n",
        "\n",
        "This tree demonstrates that the sentence is made up of a verb phrase (VP is the finest movie of the year) and a subject noun phrase (NP Everything Everywhere All at Once). The verb phrase is made up of a verb (VBZ is), an adjective (JJ the), a superlative adjective (JJS best), a noun (NN movie), and a prepositional phrase (PP of the year). The subject noun phrase is made up of four nouns (NNP Everything, NNP Everywhere, NN All, and NN at Once).\n",
        "\n",
        "\n",
        "**The dependency parsing tree for this sentence is as follows:**\n",
        "\n",
        "(Once\n",
        "  (All\n",
        "    (Everywhere\n",
        "      (Everything\n",
        "        (is (the (best movie) of the year)))\n",
        "    ))\n",
        "  )\n",
        "\n",
        "According to this sentence's dependency tree, the head word \"Once\" has two dependents: \"All\" and \"is the best movie of the year.\" The words \"Everywhere\" and \"Everything\" are reliant on the word \"All.\" There are four words that follow the phrase \"is the best movie of the year\": \"the,\" \"best,\" \"movie,\" and \"of the year.\"\n",
        "\n",
        "\n",
        "**Differences between constituency parsing trees and dependency parsing trees:**\n",
        "\n",
        "\n",
        "\n",
        "1.   Differently representing the syntactic structure of a phrase are constituency parsing trees and dependency parsing trees. While dependency parsing trees concentrate on the grammatical ties between words, constituency parsing trees concentrate on the hierarchical relationships between phrases.\n",
        "\n",
        "2.   Additionally, constituency parsing trees are frequently more expressive than dependency parsing trees. Coordination and subordination are only two examples of the syntactic phenomena that constituency parsing trees may describe. Contrarily, dependency parsing trees are frequently shorter and simpler to parse.\n",
        "\n",
        "**Applications of constituency parsing trees and dependency parsing trees:**\n",
        "\n",
        "1.   Constituency parsing trees and dependency parsing trees are used in a variety of natural language processing (NLP) applications such as machine translation, text summarization, and question answering.\n",
        "\n",
        "2.   In machine translation systems, constituency parsing trees are frequently used to translate sentences from one language to another. Text summarization systems and question answering systems both employ dependency parsing trees to extract the key points of a text and to answer questions about a text."
      ],
      "metadata": {
        "id": "m_lYN9fOKRYs"
      }
    }
  ]
}